{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T23:14:29.801718Z","iopub.execute_input":"2025-03-15T23:14:29.802139Z","iopub.status.idle":"2025-03-15T23:14:29.923629Z","shell.execute_reply.started":"2025-03-15T23:14:29.802108Z","shell.execute_reply":"2025-03-15T23:14:29.922523Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"! git clone https://github.com/Potladurthy-Sai-Praneeth/Parllel-Speculative-Decoding.git --branch master --single-branch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T22:56:57.215775Z","iopub.execute_input":"2025-03-17T22:56:57.216083Z","iopub.status.idle":"2025-03-17T22:56:57.952871Z","shell.execute_reply.started":"2025-03-17T22:56:57.216056Z","shell.execute_reply":"2025-03-17T22:56:57.951978Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'Parllel-Speculative-Decoding'...\nremote: Enumerating objects: 22, done.\u001b[K\nremote: Counting objects: 100% (16/16), done.\u001b[K\nremote: Compressing objects: 100% (14/14), done.\u001b[K\nremote: Total 22 (delta 2), reused 16 (delta 2), pack-reused 6 (from 1)\u001b[K\nReceiving objects: 100% (22/22), 553.91 KiB | 5.04 MiB/s, done.\nResolving deltas: 100% (2/2), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"! sh /kaggle/working/Parllel-Speculative-Decoding/install.sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T22:57:55.009733Z","iopub.execute_input":"2025-03-17T22:57:55.010039Z","iopub.status.idle":"2025-03-17T22:57:58.535486Z","shell.execute_reply.started":"2025-03-17T22:57:55.010016Z","shell.execute_reply":"2025-03-17T22:57:58.534335Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"! set CUDA_VISIBLE_DEVICES=0,1\n! set CUDA_LAUNCH_BLOCKING=1\n! accelerate launch /kaggle/working/Parllel-Speculative-Decoding/benchmark/eval_humaneval.py -n 3  -e output_n_3_qwen --drafts qwen-0.5b-instruct qwen-0.5b-coder qwen-0.5b --target qwen-1.5b --data_path /kaggle/working/Parllel-Speculative-Decoding/data/humaneval.jsonl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T22:58:51.596522Z","iopub.execute_input":"2025-03-17T22:58:51.596926Z"}},"outputs":[{"name":"stdout","text":"\u001b[93mLoading tokenizer of Qwen/Qwen2.5-Coder-1.5B-Instruct...\u001b[0m\ntokenizer_config.json: 100%|███████████████| 7.30k/7.30k [00:00<00:00, 26.6MB/s]\nvocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 18.9MB/s]\nmerges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 29.3MB/s]\ntokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 48.3MB/s]\n\u001b[93mLoading HumanEval data...\u001b[0m\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/kaggle/working/Parllel-Speculative-Decoding/benchmark/eval_humaneval.py\", line 120, in <module>\n[rank0]:     alg = EvalHumaneval(args)\n[rank0]:   File \"/kaggle/working/Parllel-Speculative-Decoding/benchmark/eval_humaneval.py\", line 23, in __init__\n[rank0]:     self.load_model()\n[rank0]:   File \"/kaggle/working/Parllel-Speculative-Decoding/benchmark/../src/engine.py\", line 39, in load_model\n[rank0]:     self.color_print(f\"Loading models:\\n{self.args.draft_model}\\n{self.args.target_model}\", 3)\n[rank0]: AttributeError: 'Namespace' object has no attribute 'draft_model'. Did you mean: 'draft_models'?\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/kaggle/working/Parllel-Speculative-Decoding/benchmark/eval_humaneval.py\", line 120, in <module>\n[rank1]:     alg = EvalHumaneval(args)\n[rank1]:   File \"/kaggle/working/Parllel-Speculative-Decoding/benchmark/eval_humaneval.py\", line 23, in __init__\n[rank1]:     self.load_model()\n[rank1]:   File \"/kaggle/working/Parllel-Speculative-Decoding/benchmark/../src/engine.py\", line 39, in load_model\n[rank1]:     self.color_print(f\"Loading models:\\n{self.args.draft_model}\\n{self.args.target_model}\", 3)\n[rank1]: AttributeError: 'Namespace' object has no attribute 'draft_model'. Did you mean: 'draft_models'?\n[rank0]:[W317 22:59:09.204141849 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"! set CUDA_VISIBLE_DEVICES=0,1\n! set CUDA_LAUNCH_BLOCKING=1\n! accelerate launch /kaggle/working/Parllel-Speculative-Decoding/benchmark/eval_humaneval.py -n 3  -e output_n_3_qwen --drafts qwen-0.5b-instruct qwen-0.5b-coder qwen-0.5b --target qwen-1.5b --data_path /kaggle/working/Parllel-Speculative-Decoding/data/humaneval.jsonl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T21:53:33.811183Z","iopub.execute_input":"2025-03-16T21:53:33.811494Z","iopub.status.idle":"2025-03-17T00:33:43.307645Z","shell.execute_reply.started":"2025-03-16T21:53:33.811467Z","shell.execute_reply":"2025-03-17T00:33:43.306751Z"}},"outputs":[{"name":"stdout","text":"\u001b[93mLoading tokenizer of Qwen/Qwen2.5-Coder-1.5B-Instruct...\u001b[0m\ntokenizer_config.json: 100%|███████████████| 7.30k/7.30k [00:00<00:00, 29.3MB/s]\nvocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 31.8MB/s]\nmerges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 31.1MB/s]\ntokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 44.3MB/s]\n\u001b[93mLoading HumanEval data...\u001b[0m\n\u001b[93mLoading models: \n Draft : ['Qwen/Qwen2.5-Coder-0.5B-Instruct', 'Qwen/Qwen2.5-Coder-0.5B', 'Qwen/Qwen2.5-0.5B']\n Target : Qwen/Qwen2.5-Coder-1.5B-Instruct\u001b[0m\nconfig.json: 100%|█████████████████████████████| 659/659 [00:00<00:00, 5.26MB/s]\nconfig.json: 100%|█████████████████████████████| 660/660 [00:00<00:00, 3.75MB/s]\n2025-03-16 21:53:48.840452: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-16 21:53:48.840437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-16 21:53:49.022603: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-16 21:53:49.022616: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-16 21:53:49.074924: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-03-16 21:53:49.074923: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nmodel.safetensors: 100%|██████████████████████| 988M/988M [00:04<00:00, 227MB/s]\ngeneration_config.json: 100%|██████████████████| 243/243 [00:00<00:00, 1.56MB/s]\nconfig.json: 100%|█████████████████████████████| 659/659 [00:00<00:00, 3.97MB/s]\nmodel.safetensors: 100%|██████████████████████| 988M/988M [00:04<00:00, 212MB/s]\ngeneration_config.json: 100%|███████████████████| 139/139 [00:00<00:00, 779kB/s]\nconfig.json: 100%|█████████████████████████████| 681/681 [00:00<00:00, 4.78MB/s]\nmodel.safetensors: 100%|██████████████████████| 988M/988M [00:04<00:00, 214MB/s]\nmodel.safetensors: 100%|████████████████████| 3.09G/3.09G [00:16<00:00, 188MB/s]\ngeneration_config.json: 100%|███████████████████| 138/138 [00:00<00:00, 651kB/s]\ngeneration_config.json: 100%|██████████████████| 242/242 [00:00<00:00, 1.19MB/s]\n100%|█████████| 164/164 [2:39:22<00:00, 58.31s/it]\n\u001b[90mcurrent eval mode: para_sd\u001b[0m\n\u001b[92mdraft model forward times: 251344\u001b[0m\n\u001b[92mtarget model forward times: 62836\u001b[0m\n\u001b[92mgenerate speed (tokens / second):  17.58 with std 2.428351879119873\u001b[0m\n\u001b[94mMean accepted tokens: 6.568749023081332\u001b[0m\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"! set CUDA_VISIBLE_DEVICES=0,1\n! set CUDA_LAUNCH_BLOCKING=1\n! accelerate launch /kaggle/working/Parllel-Speculative-Decoding/benchmark/eval_humaneval.py -n 3  -e output_n_3_qwen --drafts qwen-0.5b-instruct qwen-0.5b-coder qwen-0.5b --target qwen-1.5b --data_path /kaggle/working/Parllel-Speculative-Decoding/data/humaneval.jsonl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:39:15.253883Z","iopub.execute_input":"2025-03-16T17:39:15.254194Z","iopub.status.idle":"2025-03-16T20:50:53.046481Z","shell.execute_reply.started":"2025-03-16T17:39:15.254172Z","shell.execute_reply":"2025-03-16T20:50:53.045256Z"}},"outputs":[{"name":"stdout","text":"\u001b[93mLoading tokenizer of Salesforce/codegen-2B-multi...\u001b[0m\ntokenizer_config.json: 100%|███████████████████| 240/240 [00:00<00:00, 1.99MB/s]\nvocab.json: 100%|████████████████████████████| 798k/798k [00:00<00:00, 13.5MB/s]\nmerges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 39.3MB/s]\ntokenizer.json: 100%|██████████████████████| 2.11M/2.11M [00:00<00:00, 52.0MB/s]\nadded_tokens.json: 100%|███████████████████| 1.00k/1.00k [00:00<00:00, 5.96MB/s]\nspecial_tokens_map.json: 100%|████████████████| 90.0/90.0 [00:00<00:00, 572kB/s]\n\u001b[93mLoading HumanEval data...\u001b[0m\n\u001b[93mLoading models: \n Draft : ['Salesforce/codegen-350M-multi', 'Salesforce/codegen-350M-mono', 'Salesforce/codegen-350M-nl']\n Target : Salesforce/codegen-2B-multi\u001b[0m\nconfig.json: 100%|█████████████████████████| 1.00k/1.00k [00:00<00:00, 5.88MB/s]\nconfig.json: 100%|█████████████████████████████| 998/998 [00:00<00:00, 7.61MB/s]\n2025-03-16 17:39:34.102735: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-16 17:39:34.102726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-16 17:39:34.441192: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-16 17:39:34.441224: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-16 17:39:34.539947: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-03-16 17:39:34.539992: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\npytorch_model.bin: 100%|██████████████████████| 797M/797M [00:03<00:00, 231MB/s]\npytorch_model.bin:  13%|██▊                  | 755M/5.69G [00:03<00:21, 228MB/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nconfig.json: 100%|█████████████████████████████| 999/999 [00:00<00:00, 6.05MB/s]\npytorch_model.bin: 100%|██████████████████████| 797M/797M [00:03<00:00, 227MB/s]\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nconfig.json: 100%|█████████████████████████████| 997/997 [00:00<00:00, 4.25MB/s]\nmodel.safetensors: 100%|██████████████████████| 797M/797M [00:03<00:00, 218MB/s]\npytorch_model.bin: 100%|█████████████████████| 797M/797M [00:08<00:00, 99.2MB/s]\npytorch_model.bin:  73%|██████████████▋     | 4.17G/5.69G [00:19<00:11, 136MB/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\npytorch_model.bin: 100%|████████████████████| 5.69G/5.69G [00:32<00:00, 173MB/s]\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n100%|█████████| 164/164 [3:10:38<00:00, 69.75s/it]\n\u001b[90mcurrent eval mode: para_sd\u001b[0m\n\u001b[92mdraft model forward times: 274988\u001b[0m\n\u001b[92mtarget model forward times: 68747\u001b[0m\n\u001b[92mgenerate speed (tokens / second):  14.73 with std 1.8791358470916748\u001b[0m\n\u001b[94mMean accepted tokens: 5.445052093728793\u001b[0m\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"! set CUDA_VISIBLE_DEVICES=0,1\n! set CUDA_LAUNCH_BLOCKING=1\n! accelerate launch /kaggle/working/Parllel-Speculative-Decoding/benchmark/eval_mbpp.py -n 3  -e output_n_3_codegen --drafts codegen-350m-multi codegen-350m-mono codegen-350-nl --target codegen-2b --data_path /kaggle/working/Parllel-Speculative-Decoding/data/mbpp.jsonl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Coding \n## n==2\n\n### Codegen\n- draft model forward times: 239228\n- target model forward times: 59807\n- generate speed (tokens / second):  16.91 with std 2.0472264289855957\n- Mean accepted tokens: 8.378154379020287\n\n## n==3\n\n### Codegen - humaneval\n- draft model forward times: 274988\n- target model forward times: 68747\n- generate speed (tokens / second):  14.73 with std 1.8791358470916748\n- Mean accepted tokens: 5.445052093728793\n\n### Codegen - mbpp\n- draft model forward times: 359068\n- target model forward times: 89767\n- generate speed (tokens / second):  14.44 with std 2.6982927322387695\n- Mean accepted tokens: 4.025390901122515\n\n### Qwen - humaneval\n- draft model forward times: 251344\n- target model forward times: 62836\n- generate speed (tokens / second):  17.58 with std 2.428351879119873\n- Mean accepted tokens: 6.568749023081332\n\n### Qwen - mbpp\n- draft model forward times: 301296\n- target model forward times: 75324\n- generate speed (tokens / second):  18.12 with std 2.5266571044921875\n- Mean accepted tokens: 6.662622222222222","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}